# -*- coding: utf-8 -*-
"""Neural Network - AND & NOR Gate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qrcJEd9PjdQ2Y88yEZpRfvBTVbd5ltau

# Applying Neural Network for different Logic Gates

## First let's apply for AND Gate

### Importing necessary libraries
"""

import matplotlib.pyplot as plt
import numpy as np

"""### Assigning different Input values"""

# Assign Input values
feature_one = np.array([[0,0], [0,1], [1,0], [1,1]])
print(feature_one.shape)
feature_one

"""### Assigning Target output as per the Truth Table for AND gate

### Boolean Expression -- A*B -- Target outputs

### (0,1) - 0
### (0,0) - 0
### (1,0) - 0
### (1,1) - 1
"""

# Target output
target_output = np.array([[0,0,0,1]])
target_output = target_output.reshape(4,1)
print(target_output.shape)
target_output

"""### Assigning random weights"""

# Assign the weights
weights = np.array([[12.5053271],[12.5053271]]) 
print(weights.shape)
weights

"""### Adding Bias value ( if needed - as per the input value )

### Also assigning Standard Learning Rate
"""

#Adding Bias values and Assigning a Learning rate
#bias weight
bias = -19.04631113 
#neural network

lr = 0.05 # Standard Rate

"""### Using Sigmoid Function for converting the value in the range of (0,1)
### Sigmoid Function serves as an activation function in our neural network training.
"""

# Derivative of sigmoid function
def sigmoid(x):
    return 1/(1+np.exp(-x))

def sigmoid_der(x):
    return sigmoid(x)*(1-sigmoid(x))

"""### Following is the logic for applying FeedForward and Backpropagation to get the target output.

### Epoch - Generally indicates the number of passes of the entire training dataset the machine learning algorithm has completed
"""

# The main logic for predicting output and updating the weight values:

for epoch in range(10000):
    inputs = feature_one

    # feedforward input
    in_o = np.dot(feature_one, weights) + bias

    #feedforward output
    out_o = sigmoid(in_o)


    # backpropagation calculating error
    error = out_o - target_output

    # applying formula
    print(error.sum())

# Calculating derivative
    derror_douto = error
    douto_dino = sigmoid_der(out_o)

    # Multiplying individual derivatives
    deriv = derror_douto * douto_dino

    # Multiplying with the 3rd individual derivative
    # finding the transpose of input features
    inputs = feature_one.T
    deriv_final=np.dot(inputs,deriv)
    
    #updating the weight values
    weights -= lr * deriv_final

    # updating the bias weight value
    for i in deriv:
        bias -= lr * i

"""### Now we get the Updated values of Weights and bias to achieve the target output"""

# Updated values of weights and bias as per the calculations
print(weights)
print(bias)

"""### Finally, we assign the input values and getting the target output one by one for each & every input. """

#taking input
#pt=np.array([0,1]) 
pt=np.array([1,1])
# 1st step
result1= np.dot(pt,weights)+bias

# 2nd step
result2=sigmoid(result1)
    
#print result

print(result2)
print("Rounding off - ", round(result2[0]))
print("Close to 1 - which is our target output for [1,1]")

"""## As we can see if we keep Updating the values for weights and bias, we can get our target output

# Similarly, Let's apply the same for NOR Gate
"""

# Importing necessary libraries
import matplotlib.pyplot as plt
import numpy as np

# Assign Input values
feature_two = np.array([[0,0], [0,1], [1,0], [1,1]])
print(feature_two.shape)
feature_two

"""## Assigning Target output as per the Truth Table for NOR gate

## Boolean Expression -- (A+B)^Transpose -- Target outputs

## (0,0) - 1
## (0,1) - 0
## (1,0) - 0
## (1,1) - 0
"""

# Target output
target_output = np.array([[1,0,0,0]])
target_output = target_output.reshape(4,1)
print(target_output.shape)
target_output

"""### Assigning Random weights """

# Assign the weights ( random )
weights = np.array([[8.348],[8.348]])
print(weights.shape)
weights

#Adding Bias values and Assigning a Learning rate
bias = 3.592
#neural network

# Standard Learning Rate
lr = 0.05

# Derivative of sigmoid function
def sigmoid(x):
    return 1/(1+np.exp(-x))

def sigmoid_der(x):
    return sigmoid(x)*(1-sigmoid(x))

"""### Sometimes we can increase the Number of Epochs for Better Results"""

# The main logic for predicting output and updating the weight values:

for epoch in range(30000):
    inputs = feature_two

    # feedforward input
    in_o = np.dot(feature_two, weights) + bias

    #feedforward output
    out_o = sigmoid(in_o)


    # backpropagation calculating error
    error = out_o - target_output

    # applying formula
    print(error.sum())

# Calculating derivative
    derror_douto = error
    douto_dino = sigmoid_der(out_o)

    # Multiplying individual derivatives
    deriv = derror_douto * douto_dino

    # Multiplying with the 3rd individual derivative
    # finding the transpose of input features
    inputs = feature_two.T
    deriv_final=np.dot(inputs,deriv)
    
    #updating the weight values
    weights -= lr * deriv_final

    # updating the bias weight value
    for i in deriv:
        bias -= lr * i

print(weights)
print(bias)

#taking input

pt=np.array([0,1])
# 1st step
result1= np.dot(pt,weights)+bias

# 2nd step
result2=sigmoid(result1)
    
#print result
print(result2)
print("Rounding off - ", round(result2[0]))
print("Close to 0 - which is our Target output for [0,1]")